# 22,223
crime$reportdatetime    <- ymd_hms(crime$reportdatetime, tz = "UTC")
crime$lastmodifieddate  <- ymd_hms(crime$lastmodifieddate, tz = "UTC")
crime$end_date          <- ymd_hms(crime$end_date, tz = "UTC")
crime %>% summarise(min(reportdatetime), max(reportdatetime))
uber.x2.roll %>% summarise(min(timestamp.half), max(timestamp.half))
## limit crime data to be inside uber dates:
# format dates:
nrow(crime)
## look at offense scatter across DC
ggplot(crime, aes(x=long, y=lat, color=offense)) + geom_point() + coord_equal()
crime %>%
dplyr::summarise(count=n()) %>%
arrange(desc(count))
#                THEFT/OTHER   868 0.3895870736
#                    ROBBERY   218 0.0978456014
#        MOTOR VEHICLE THEFT   135 0.0605924596
#                  SEX ABUSE    15 0.0067324955
#                      ARSON     2 0.0008976661
# extract items of interest from crime table
######################################################################
crime$start_date        <- ymd_hms(crime$start_date, tz = "UTC")
## add closest uber point to crime table along with its distance
for (i in 1:nrow(crime_list)) {
dists <- distm(crime.point, lat.long[,c("longitude","latitude")])
min_uber_point <- lat.long[,][min_uber_dist,]
crime_list$uber.long[i] <- min_uber_point[,2]
crime_list$uber.location[i] <- min_uber_point[,3]
}
crime<-crime[crime$reportdatetime>min(uber.x2.roll$timestamp.half) &
# 2,228 total crimes in timeframe
## look at offense type distribution
mutate(freq = count / sum(count)) %>%
#               THEFT F/AUTO   719 0.3227109515
#                   BURGLARY   114 0.0511669659
# Probably only want to concentrate on non property crimes
## find nearest uber point for each crime
######################################################################
min_uber_dist <- which(dists==min(dists))
## check date overlap: crime data should be longer timeframe than uber:
crime <- read.csv("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/Data/Crime_Incidents__2016.csv")
crime$reportdatetime<max(uber.x2.roll$timestamp.half),]
group_by(offense)  %>%
# ASSAULT W/DANGEROUS WEAPON   147 0.0659784560
crime_list<- crime[,c("long", "lat", "reportdatetime", "objectid")]
crime.point <- crime_list[i,1:2]
crime_list$uber.dist[i] <- min_uber_dist
avg.high  = mean(high_estimate, na.rm=TRUE))
#                   HOMICIDE    10 0.0044883303
crime_list$uber.lat[i] <- min_uber_point[,1]
#                     offense count         freq
nrow(crime)
## use distm to create matrix of closest uber points
setwd("/Users/bradyfowler/Downloads/Census")
library(maptools)
library(ggplot2)
tract <- readShapePoly('Census_Tracts__2010.shp')
cdata <- read.csv('Census_Tracts__2010.csv')
tract_geom <- fortify(tract, region = "GEOID")
tract_poly <- merge(tract_geom, cdata, by.x = "id", by.y = "GEOID")
FAGI2013 <- ggplot(tract_poly, aes(long, lat, group = group, fill = FAGI_MEDIAN_2013)) +
geom_polygon() +
scale_fill_gradient(low = "skyblue1", high = "blue4",
breaks = c(25000, 50000, 75000, 100000, 125000),
labels = c("$25K", "$50", "$75", "$100K", "$125K"),
guide = guide_legend(title = "Median Income")) +
coord_equal()
head(tract_poly)
# load packages
library(dplyr); library(ggplot2); library(lubridate); library(geosphere); library(plyr)
####################################################################################
## begin matching uber data to crime data
####################################################################################
## import latitude and longitude locations for each coordinate location in uber data
lat.long <- read.csv("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/Data/awsLocations.csv")
#plot to see distribution
ggplot(lat.long, aes(x=longitude, y=latitude)) + geom_point() + coord_equal()
## import uberx data
setwd("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/output/")
load("uberx.Rda")
## join lat/long details into uberx
## remove unnecessary columns and cast the timestamps
uber.x2  <- left_join(uber.x, lat.long, by=c("start_location_id"="locations")) %>%
select(-end_location_id, -expected_wait_time, -product_type)
uber.x2$timestamp <- ymd_hms(uber.x2$timestamp, tz = "UTC")
head(uber.x2)
## roll uber data up to the nearest hour
uber.x2$timestamp.half <- uber.x2$timestamp
minute(uber.x2$timestamp.half)<- floor(minute(uber.x2$timestamp)/30)*30
second(uber.x2$timestamp.half)<-0
# groupby timestamp and location and average surge/low/high
uber.x2.roll <- uber.x2 %>%
dplyr::group_by(timestamp.half, start_location_id, latitude, longitude) %>%
dplyr::summarise(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
## import crime data and fix column names
crime <- read.csv("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/Data/Crime_Incidents__2016.csv")
colnames(crime)<-tolower(c("long", "lat", colnames(crime)[3:length(crime)]))
nrow(crime)
# 22,223
# format dates:
crime$reportdatetime    <- ymd_hms(crime$reportdatetime, tz = "UTC")
crime$lastmodifieddate  <- ymd_hms(crime$lastmodifieddate, tz = "UTC")
crime$start_date        <- ymd_hms(crime$start_date, tz = "UTC")
crime$end_date          <- ymd_hms(crime$end_date, tz = "UTC")
## check date overlap: crime data should be longer timeframe than uber:
crime %>% summarise(min(reportdatetime), max(reportdatetime))
uber.x2.roll %>% summarise(min(timestamp.half), max(timestamp.half))
## limit crime data to be inside uber dates:
crime<-crime[crime$reportdatetime>min(uber.x2.roll$timestamp.half) &
crime$reportdatetime<max(uber.x2.roll$timestamp.half),]
nrow(crime)
# 2,228 total crimes in timeframe
## look at offense scatter across DC
ggplot(crime, aes(x=long, y=lat, color=offense)) + geom_point() + coord_equal()
## look at offense type distribution
crime %>%
group_by(offense)  %>%
dplyr::summarise(count=n()) %>%
mutate(freq = count / sum(count)) %>%
arrange(desc(count))
#                     offense count         freq
#                THEFT/OTHER   868 0.3895870736
#               THEFT F/AUTO   719 0.3227109515
#                    ROBBERY   218 0.0978456014
# ASSAULT W/DANGEROUS WEAPON   147 0.0659784560
#        MOTOR VEHICLE THEFT   135 0.0605924596
#                   BURGLARY   114 0.0511669659
#                  SEX ABUSE    15 0.0067324955
#                   HOMICIDE    10 0.0044883303
#                      ARSON     2 0.0008976661
# Probably only want to concentrate on non property crimes
# extract items of interest from crime table
crime_list<- crime[,c("long", "lat", "reportdatetime", "objectid")]
######################################################################
## find nearest uber point for each crime
## use distm to create matrix of closest uber points
## add closest uber point to crime table along with its distance
######################################################################
for (i in 1:nrow(crime_list)) {
crime.point <- crime_list[i,1:2]
dists <- distm(crime.point, lat.long[,c("longitude","latitude")])
min_uber_dist <- which(dists==min(dists))
min_uber_point <- lat.long[,][min_uber_dist,]
crime_list$uber.lat[i] <- min_uber_point[,1]
crime_list$uber.long[i] <- min_uber_point[,2]
crime_list$uber.location[i] <- min_uber_point[,3]
crime_list$uber.dist[i] <- min_uber_dist
}
# whats the distribution of distances for the "nearest"
summary(crime_list$uber.dist)
hist(crime_list$uber.dist)
plot(ecdf(crime_list$uber.dist))
## most crimes can be matched to a uber call within 200 meters ~80%
head(lat.long)
head(tract_poly)
CRS(pointsProj4String)
CRS(tract_poly)
CRS(tract)
proj4string(tract)
proj4string(cdata)
proj4string(tract)
tract
dc <- readOGR(dsn="/Users/bradyfowler/Downloads/Census", layer="Census_Tracts")
# import packages that Professor Gerber used
library(ks)
library(RColorBrewer)
library(zoo)
library(rgdal)
library(maptools)
library(lubridate)
library(MASS)
# import dplyr & plyr for data manipulation and ggplot2 for visualization
library(dplyr)
library(plyr)
library(ggplot2)
dc <- readOGR(dsn="/Users/bradyfowler/Downloads/Census", layer="Census_Tracts")
dc <- readOGR(dsn="/Users/bradyfowler/Downloads/Census", layer="Census_Tracts__2010")
proj4string(dc)
CRS(proj4string(dc))
head(tract_poly)
head(tract_poly)
tract_poly[,c("long","lat")]
sp.points = SpatialPoints(tract_poly[,c("long","lat")], proj4string=CRS(proj4string(dc)))
sp.points
points.in.boundary = !is.na(over(sp.points, boundary)$OBJECTID)
points.in.boundary = !is.na(over(sp.points, dc)$OBJECTID)
points.in.boundary
head(dc)
proj4string
dc
head(tract_geom)
unique(tract_geom$piece)
tract_geom$piece
head(tract_geom)
unique(tract_geom$id)
length(unique(tract_geom$id))
head(tract_geom)
tract_poly
View(tract_poly)
library(spatialEco)
install.packages(spatialEco)
install.packages("spatialEco")
library(spatialEco)
dc
new_shape <- point.in.poly(tract_poly[,c("long","lat")], dc)
new_shape <- point.in.poly(tract_poly[,c("long","lat")], proj4string(dc))
new_shape <- point.in.poly(tract_poly[,c("long","lat")], CRS(proj4string(dc)))
sp.points = SpatialPoints(tract_poly[,c("long","lat")], proj4string=CRS(proj4string(dc)))
new_shape <- point.in.poly(tract_poly[,c("long","lat")], sp.points)
sp.points = SpatialPoints(tract_poly[,c("long","lat")], proj4string=CRS(proj4string(dc)))
new_shape <- point.in.poly(tract_poly[,c("long","lat")], sp.points)
new_shape <- point.in.poly(tract_poly[,c("long","lat")][1], sp.points)
??point.in.poly
tract_poly[,c("long","lat")][1]
tract_poly[,c("long","lat")][,1]
tract_poly[,c("long","lat")][1,]
head(tract_geom)
tract_geom[tract_geom$id==11001000100]
tract_geom[tract_geom$id==11001000100,]
head(tract_geom)
point.in.poly(tract_poly[,c("long","lat")][1,], tract_geom[tract_geom$id==11001000100,c("long","lat")])
tract_geom[tract_geom$id==11001000100,c("long","lat")]
pnts = expand.grid(x=seq(1,6,0.1),y=seq(1,6,0.1))
polypnts = cbind(x=c(2,3,3.5,3.5,3,4,5,4,5,5,4,3,3,3,2,2,1,1,1,1,2),
y=c(1,2,2.5,2,2,1,2,3,4,5,4,5,4,3,3,4,5,4,3,2,2))
#plot the polygon and all points to be checked
plot(rbind(polypnts, pnts))
polygon(polypnts,col='#99999990')
#create check which points fall within the polygon
out = pnt.in.poly(pnts,polypnts)
head(out)
#identify points not in the polygon with an X
points(out[which(out$pip==0),1:2],pch='X')
plot(rbind(polypnts, pnts))
polygon(polypnts,col='#99999990')
#create check which points fall within the polygon
out = pnt.in.poly(pnts,polypnts)
head(out)
#identify points not in the polygon with an X
points(out[which(out$pip==0),1:2],pch='X')
out = point.in.poly(pnts,polypnts)
pnt.in.poly(tract_poly[,c("long","lat")][1,], tract_geom[tract_geom$id==11001000100,c("long","lat")])
library("‘SDMTools’")
library("SDMTools")
out = pnt.in.poly(pnts,polypnts)
head(out)
points(out[which(out$pip==0),1:2],pch='X')
pnts = expand.grid(x=seq(1,6,0.1),y=seq(1,6,0.1))
polypnts = cbind(x=c(2,3,3.5,3.5,3,4,5,4,5,5,4,3,3,3,2,2,1,1,1,1,2),
y=c(1,2,2.5,2,2,1,2,3,4,5,4,5,4,3,3,4,5,4,3,2,2))
#plot the polygon and all points to be checked
plot(rbind(polypnts, pnts))
polygon(polypnts,col='#99999990')
library("SDMTools")
#create check which points fall within the polygon
out = pnt.in.poly(pnts,polypnts)
head(out)
#identify points not in the polygon with an X
points(out[which(out$pip==0),1:2],pch='X')
head(out)
points(out[which(out$pip==1),1:2],pch='X')
pnt.in.poly(tract_poly[,c("long","lat")][1,], tract_geom[tract_geom$id==11001000100,c("long","lat")])
pnt.in.poly(tract_poly[,c("long","lat")], tract_geom[tract_geom$id==11001000100,c("long","lat")])
geom_ids<-unique(tract_geom$id)
geom_ids<-data.frame(unique(tract_geom$id))
geom_ids[,1]
View(geom_ids)
colnames(geom_ids)<-"unique.id"
geom_ids[1]
geom_ids[1,]
geom_ids[2]
geom_ids[2,]
lat.long[,c("long","lat")]
lat.long[,c("longitude","latitude")]
tract_points_in_geom <- pnt.in.poly(lat.long[,c("longitude","latitude")], tract_geom[tract_geom$id==geom_ids[2,],c("long","lat")])
tract_points_in_geom
tract_points_in_geom[tract_points_in_geom$pip==1,]
lat.long.geoid <- data.frame()
rbind(tract_points_in_geom[tract_points_in_geom$pip==1,], geom_ids[2,])
cbind(tract_points_in_geom[tract_points_in_geom$pip==1,], geom_ids[2,])
geom_ids<-data.frame(unique(tract_geom$id))
colnames(geom_ids)<-"unique.id"
lat.long.geoid <- data.frame()
for (i in 1:nrow(geom_ids){
tract_points_in_geom <- pnt.in.poly(lat.long[,c("longitude","latitude")], tract_geom[tract_geom$id==geom_ids[i,],c("long","lat")])
lat.long.geoid <- cbind(tract_points_in_geom[tract_points_in_geom$pip==1,], geom_ids[i,])
}
geom_ids<-data.frame(unique(tract_geom$id))
colnames(geom_ids)<-"unique.id"
lat.long.geoid <- data.frame()
for (i in 1:nrow(geom_ids)){
tract_points_in_geom <- pnt.in.poly(lat.long[,c("longitude","latitude")], tract_geom[tract_geom$id==geom_ids[i,],c("long","lat")])
lat.long.geoid <- cbind(tract_points_in_geom[tract_points_in_geom$pip==1,], geom_ids[i,])
}
lat.long.geoid
geom_ids<-data.frame(unique(tract_geom$id))
colnames(geom_ids)<-"unique.id"
lat.long.geoid <- data.frame()
for (i in 1:nrow(geom_ids)){
current.id <- geom_ids[i,]
tract_points_in_geom <- pnt.in.poly(lat.long[,c("longitude","latitude")], tract_geom[tract_geom$id==current.id,c("long","lat")])
lat.long.geoid <- cbind(tract_points_in_geom[tract_points_in_geom$pip==1,], current.id)
}
lat.long.geoid
tract_geom[tract_geom$id==current.id,c("long","lat")]
nrow(geom_ids)
tract_geom[tract_geom$id==179, c("long","lat")]
tract_geom[tract_geom$id=179, c("long","lat")]
tract_geom[tract_geom$id==179,]
current.id <- geom_ids[179,]
current.id
tract_geom[tract_geom$id==11001011100, c("long","lat")]
pnt.in.poly(lat.long[,c("longitude","latitude")], tract_geom[tract_geom$id==11001011100, c("long","lat")])
tract_points_in_geom[tract_points_in_geom$pip==1,]
cbind(tract_points_in_geom[tract_points_in_geom$pip==1,], current.id)
geom_ids<-data.frame(unique(tract_geom$id))
colnames(geom_ids)<-"unique.id"
lat.long.geoid <- data.frame()
for (i in 1:nrow(geom_ids)){
current.id <- geom_ids[i,]
tract_points_in_geom <- pnt.in.poly(lat.long[,c("longitude","latitude")], tract_geom[tract_geom$id==current.id, c("long","lat")])
lat.long.geoid <- rbind(lat.long.geoid, cbind(tract_points_in_geom[tract_points_in_geom$pip==1,], current.id))
}
lat.long.geoid
View(lat.long.geoid)
head(tract_poly)
FAGI2013
FAGI2013 + geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude,
group = current.id, fill = latitude)) + coord_equal()
FAGI2013 + geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude,
group = current.id, fill = current.id)) + coord_equal()
FAGI2013 + geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude,
group = current.id, fill = factor(current.id))) + coord_equal()
factor(current.id)
FAGI2013 + geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude,
group = current.id, fill = factor(lat.long.geoid$current.id))) + coord_equal()
FAGI2013 + geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude,
group = current.id) + coord_equal()
)
FAGI2013 + geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude, group = current.id)) + coord_equal()
FAGI2013
ggplot()+geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude, group = current.id)) + coord_equal()
ggplot()+geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude, fill = current.id)) + coord_equal()
ggplot()+geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude, color = current.id)) + coord_equal()
FAGI2013 +  geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude, color = current.id)) + coord_equal()
FAGI2013 +  geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude, group = current.id, color = current.id)) + coord_equal()
ggplot()+geom_point(data = lat.long.geoid, aes(x = longitude, y = latitude, group = current.id, color = current.id)) + coord_equal()
uberx.analysis <- dplyr::left_join(uber.x2.roll,crime_list,
by=c("timestamp.half"="uber.time",
"start_location_id"="uber.location"))
uberx.analysis$has.crime <- ifelse(is.na(uberx.analysis$objectid)==TRUE,0,1)
uberx.analysis <- dplyr::left_join(uber.x2.roll,crime_list,
by=c("start_location_id"="uber.location"))
uberx.analysis$has.crime <- ifelse(is.na(uberx.analysis$objectid)==TRUE,0,1)
uberx.analysis %>%
dplyr::group_by(has.crime)  %>%
dplyr::summarise(count=n()) %>%
mutate(freq = count*100 / sum(count)) %>%
arrange(desc(count))
head(uberx.analysis)
uberx.analysis <- left_join(uberx.analysis, lat.long.geoid, by=("lat"="lat", "long"="long"))
uberx.analysis <- left_join(uberx.analysis, lat.long.geoid, by=("lat"="lat" and "long"="long"))
uberx.analysis <- left_join(uberx.analysis, lat.long.geoid, by=("lat"="lat", "long"="long"))
uberx.analysis <- left_join(uberx.analysis, lat.long.geoid)
head(uberx.analysis)
View(uberx.analysis)
cdata <- read.csv('Census_Tracts__2010.csv')
head(cdata)
uberx.analysis <- left_join(uberx.analysis, cdata, by=c("current.id"="GEOID"))
head(cdata)
View(cdata)
uberx.analysis <- dplyr::left_join(uber.x2.roll,crime_list,
by=c(##"timestamp.half"="uber.time",
"start_location_id"="uber.location"))
uberx.analysis$has.crime <- ifelse(is.na(uberx.analysis$objectid)==TRUE,0,1)
uberx.analysis %>%
dplyr::group_by(has.crime)  %>%
dplyr::summarise(count=n()) %>%
mutate(freq = count*100 / sum(count)) %>%
arrange(desc(count))
# only .6% of uber times have a crime occuring around them
# adapted from Lev's code:
library(SDMTools); library(maptools)
tract <- readShapePoly('Census_Tracts__2010.shp')
cdata <- read.csv('Census_Tracts__2010.csv')
tract_geom <- fortify(tract, region = "GEOID")
# for each uber point, figure out which geoid it fits in
# create new dataframe with lat, long of uber and geo ID
geom_ids<-data.frame(unique(tract_geom$id))
colnames(geom_ids)<-"unique.id"
lat.long.geoid <- data.frame()
for (i in 1:nrow(geom_ids)){
current.id <- geom_ids[i,]
tract_points_in_geom <- pnt.in.poly(lat.long[,c("longitude","latitude")], tract_geom[tract_geom$id==current.id, c("long","lat")])
lat.long.geoid <- rbind(lat.long.geoid, cbind(tract_points_in_geom[tract_points_in_geom$pip==1,], current.id))
}
library(SDMTools); library(maptools)
tract <- readShapePoly('Census_Tracts__2010.shp')
cdata <- read.csv('/Users/bradyfowler/Downloads/Census/Census_Tracts__2010.csv')
tract_geom <- fortify(tract, region = "GEOID")
# for each uber point, figure out which geoid it fits in
# create new dataframe with lat, long of uber and geo ID
geom_ids<-data.frame(unique(tract_geom$id))
colnames(geom_ids)<-"unique.id"
lat.long.geoid <- data.frame()
for (i in 1:nrow(geom_ids)){
current.id <- geom_ids[i,]
tract_points_in_geom <- pnt.in.poly(lat.long[,c("longitude","latitude")], tract_geom[tract_geom$id==current.id, c("long","lat")])
lat.long.geoid <- rbind(lat.long.geoid, cbind(tract_points_in_geom[tract_points_in_geom$pip==1,], current.id))
}
uberx.analysis <- left_join(uberx.analysis, lat.long.geoid)
uberx.analysis <- left_join(uberx.analysis, cdata, by=c("current.id"="GEOID"))
cdata$GEOID <- factor(cdata$GEOID)
uberx.analysis <- left_join(uberx.analysis, cdata, by=c("current.id"="GEOID"))
head(uberx.analysis)
## look at surge distribution
ggplot(uberx.analysis, aes(x=has.crime, y=avg.surge, group=has.crime, fill=has.crime)) + geom_violin()
train.indices = sample(1:nrow(uberx.analysis), as.integer(nrow(uberx.analysis) * 0.75))
log.fit = glm(has.crime ~ avg.surge+FAGI_MEDIAN_2013+factor(dow), data = uberx.analysis[train.indices, ], family="binomial")
summary(log.fit)
log.fit = glm(has.crime ~ avg.surge+FAGI_MEDIAN_2013, data = uberx.analysis[train.indices, ], family="binomial")
summary(log.fit)
log.fit$coefficients
predictions = predict.glm(log.fit, newdata = uberx.analysis[-train.indices, ], type="response")
num.correct = sum(predictions == uberx.analysis[-train.indices,]$has.crime)
accuracy = num.correct / nrow(uberx.analysis[-train.indices, ])
accuracy
num.correct
predictions
head(uber.x2.roll)
uber.x2.roll.notime <- uber.x2 %>%
dplyr::group_by(start_location_id, latitude, longitude) %>%
dplyr::summarise(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
uberx.analysis <- dplyr::left_join(uber.x2.roll.notime,crime_list,
by=c(##"timestamp.half"="uber.time",
"start_location_id"="uber.location"))
head(uberx.analysis)
View(uberx.analysis)
uberx.analysis <- uberx.analysis %>%
dplyr::group_by(start_location_id, latitude, longitude, avg.surge, avg.low, avg.high) %>%
dplyr::summarise(num.crime = n_distinct(objectid))
uberx.analysis
# adapted from Lev's code:
library(SDMTools); library(maptools)
tract <- readShapePoly('Census_Tracts__2010.shp')
cdata <- read.csv('/Users/bradyfowler/Downloads/Census/Census_Tracts__2010.csv')
tract_geom <- fortify(tract, region = "GEOID")
# for each uber point, figure out which geoid it fits in
# create new dataframe with lat, long of uber and geo ID
geom_ids<-data.frame(unique(tract_geom$id))
colnames(geom_ids)<-"unique.id"
lat.long.geoid <- data.frame()
for (i in 1:nrow(geom_ids)){
current.id <- geom_ids[i,]
tract_points_in_geom <- pnt.in.poly(lat.long[,c("longitude","latitude")], tract_geom[tract_geom$id==current.id, c("long","lat")])
lat.long.geoid <- rbind(lat.long.geoid, cbind(tract_points_in_geom[tract_points_in_geom$pip==1,], current.id))
}
uberx.analysis <- left_join(uberx.analysis, lat.long.geoid, by = c("latitude", "longitude"))
cdata$GEOID <- factor(cdata$GEOID)
uberx.analysis <- left_join(uberx.analysis, cdata, by=c("current.id"="GEOID"))
head(uberx.analysis)
## look at surge distribution
ggplot(uberx.analysis, aes(x=has.crime, y=avg.surge, group=has.crime, fill=has.crime)) + geom_violin()
ggplot(uberx.analysis, aes(x=num.crime, y=avg.surge, group=has.crime, fill=has.crime)) + geom_violin()
ggplot(uberx.analysis, aes(x=num.crime, y=avg.surge, group=has.crime, fill=num.crime)) + geom_violin()
ggplot(uberx.analysis, aes(x=num.crime, y=avg.surge, group=num.crime, fill=num.crime)) + geom_violin()
ggplot(uberx.analysis, aes(x=avg.surge, y=num.crime, group=num.crime, fill=num.crime)) + geom_violin()
ggplot(uberx.analysis, aes(x=num.crime, y=avg.surge, group=num.crime, fill=num.crime)) + geom_violin()
pairs(uberx.analysis)
colnames(uberx.analysis)
pairs(uberx.analysis[,c("avg.surge", "num.crime", "FAGI_MEDIAN_2013")])
train.indices = sample(1:nrow(uberx.analysis), as.integer(nrow(uberx.analysis) * 0.75))
log.fit = lm(num.crime ~ avg.surge+FAGI_MEDIAN_2013, data = uberx.analysis[train.indices, ], family="binomial")
summary(log.fit)
predictions = predict(log.fit, newdata = uberx.analysis[-train.indices, ], type="response")
lm = lm(num.crime ~ avg.surge+FAGI_MEDIAN_2013, data = uberx.analysis[train.indices, ], family="binomial")
lm = lm(num.crime ~ avg.surge+FAGI_MEDIAN_2013, data = uberx.analysis[train.indices, ])
summary(lm)
# predict on held-out 25% and evaluate raw accuracy
predictions = predict(log.fit, newdata = uberx.analysis[-train.indices, ], type="response")
predictions
num.correct = sum(predictions == uberx.analysis[-train.indices,]$has.crime)
num.correct = sum(predictions == uberx.analysis[-train.indices,]$num.crime)
accuracy = num.correct / nrow(uberx.analysis[-train.indices, ])
accuracy
num.correct = sum((predictions-uberx.analysis[-train.indices,]$num.crime)<2)
accuracy = num.correct / nrow(uberx.analysis[-train.indices, ])
accuracy
num.correct = sum((predictions-uberx.analysis[-train.indices,]$num.crime)<4)
accuracy = num.correct / nrow(uberx.analysis[-train.indices, ])
accuracy
summary(lm)
for (i in 1:nrow(crime_list)) {
# pull current crime details
crime.point <- crime_list[i,c("uber.location", "reportdatetime")]
# limit to crimes within an hour after
potential_ubers <- uber.x2.roll[uber.x2.roll$start_location_id==crime.point[,1],] %>%
dplyr::mutate(diff = difftime(timestamp.half, crime.point[,2], units="hours")) %>%
dplyr::filter(diff>0 & diff<=1)
# select timestamp of soonest crime and assign to crime table
crime_list$uber.time[i] <- potential_ubers[potential_ubers$diff==min(potential_ubers$diff), c("timestamp.half")]
crime_list$uber.timediff[i]<-min(potential_ubers$diff)
}
crime_list$uber.time<-as.POSIXct(as.numeric(crime_list$uber.time), origin = "1970-01-01", tz = "UTC")
# whats the distribution of distances for the "soonest"
summary(crime_list$uber.timediff)
hist(crime_list$uber.timediff)
head(tract_poly)
