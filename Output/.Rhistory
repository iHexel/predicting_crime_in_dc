# We can infer that the chemist's hypothesized relationship is incorrect and there
# is no statistically significant linear relationship between the ratio of inlet oxygen/methanol
# and percent conversion. The scatter plot shows that this relationship might better be modeled with
# another statistical learning technique.
##########################################
# 2.30
##########################################
# instructions say to note any sources used - i did not use any sources outside
# the in-console R help documentation for atanh.
head(p2.12)
# a. Find correlation between steam usage and avg. ambient temp
plot(p2.12$temp, p2.12$usage, pch=20, cex=0.3)
lm_2.30 <- lm(p2.12$usage~p2.12$temp)
abline(lm_2.30)
summary(lm_2.30)
anova(lm_2.30)
# Multiple R-squared:  0.9999
# Similarly, cor(p2.12$temp, p2.12$usage) gives a correlation of 0.9999326
# The correlation is approximately 1
# b. Test the hypothesis: Ho: p = 0, Ha p =/= 0
# From the book: to = r*sqrt(n-2) / sqrt(1-r^2)
t_0 <- sqrt(summary(lm_2.30)$r.squared)*sqrt(nrow(p2.12)-2) / sqrt(1-summary(lm_2.30)$r.squared)
t_0
# 272.255
# Critical value:
qt(.975,nrow(p2.12)-2)
# 2.228139
# # 272.255 is > 2.228139 so we reject the null hypothesis and assert that p =/=0
# Could also have used t-value from regression results (also 272.255) with p-value 2e-16
# c. Test the hypothesis: Ho: p = 0.5, Ha p =/= 0.5
# Book notes that for moderately large samples we can use a z-test,
# with 12 observations
# Z0 <- (arctanh(r) - arctanh(po))*((n - 3)**.5)
# Z0 <- (arctanh(~.99) - arctanh(.5))*((n - 3)**.5)
Z_0 <- (atanh(sqrt(summary(lm_2.30)$r.squared)) - atanh(0.5))*((nrow(p2.12) - 3)**.5)
Z_0
# 13.79796 << Please note I used all available significant figures
# rounding the square root of R2 to fewer digits and taking the arctanh can give smaller
# values of Z0 but all are larger than Za/2
# We reject H0 if abs(Z_0) > Z(a/2)
qnorm(0.975,mean=0,sd=1)
# 1.959964
# 13.79796 is > 1.959964 so we reject H0.
# d. Find a 99% CI for p
# Book gives us confidence interval formula:
# lower <- tanh(arctanh(r) - ((Za/2)/sqrt(n-3)))
# upper <- tanh(arctanh(r) + ((Za/2)/sqrt(n-3)))
z_a2 <- qnorm(0.975,mean=0,sd=1)
# lower
tanh(atanh(sqrt(summary(lm_2.30)$r.squared)) - ((z_a2)/sqrt(nrow(p2.12)-3)))
# upper
tanh(atanh(sqrt(summary(lm_2.30)$r.squared)) + ((z_a2)/sqrt(nrow(p2.12)-3)))
# (0.9997509, 0.9999817) <- please note that using fewer significant digits in the arctanh calulation
# will yield a slightly larger CI (0.9635534, 0.9972828)
#tanh(atanh(.99) - ((z_a2)/sqrt(nrow(p2.12)-3)))
#tanh(atanh(.99) + ((z_a2)/sqrt(nrow(p2.12)-3)))
pF()
qf(.95, df1=5, df2=2)
?qf
qf(.95, n=25, df1=2)
qf(.95, n=25, df1=2, df2=1)
qf(.95, n=25, df1=2, df2=2)
qf(.95, df1=2, df2=2)
qf(.95, df1=23, df2=2)
qf(.95, df1=2, df2=22)
qf(.95, df1=2, df2=22)
(.9*(25-3))/((2*.1))
3.44/11
1-3.44/11
0.3127273/(1-3.44/11)
1+3.44/11
0.3127273/(1+3.44/11)
library(gdata)
##########################################
# 3.1
##########################################
## use table b1
# a. Fit a multiple linear reg of # of games won against yardage,
# percent rushing plays, opponents yards rushing (y, x2, x7, x8)
b1 = read.xls('/Users/bradyfowler/Documents/Fall Semester/Modeling_6021/HW3/data-table-B1.xls')
head(b1)
lm_3.1 <- lm(y~x2 + x7 + x8, data=b1)
summary(lm_3.1)
# use package to import xls files
library(gdata)
##########################################
# 3.1
##########################################
## use table b1
# a. Fit a multiple linear reg of # of games won against yardage,
# percent rushing plays, opponents yards rushing (y, x2, x7, x8)
b1 = read.xls('/Users/bradyfowler/Documents/Fall Semester/Modeling_6021/HW3/data-table-B1.xls')
head(b1)
lm_3.1 <- lm(y~x2 + x7 + x8, data=b1)
summary(lm_3.1)
#                  Estimate  Std. Error     t value   Pr(>|t|)
#  (Intercept)    -1.808372   7.900859      -0.229    0.820899
#     x2           0.003598   0.000695       5.177    2.66e-05 ***
#     x7           0.193960   0.088233       2.198    0.037815 *
#     x8          -0.004816   0.001277      -3.771    0.000938 ***
# Residual standard error: 1.706 on 24 degrees of freedom
# Multiple R-squared:  0.7863,	Adjusted R-squared:  0.7596
# F-statistic: 29.44 on 3 and 24 DF,  p-value: 3.273e-08
## y = -1.808372 + 0.003598x2 + 0.193960x7 - 0.004816x8
# b. Construct an ANOVA table and test for sig reg
anova(lm_3.1)
(sum(anova(lm_3.1)[1:3,2]) - sum(anova(lm(y~x2 + x8, data=b1))[1:2,2])) / (sum(lm_3.1$residuals^2)/anova(lm_3.1)[4,1])
confint(lm_3.1, 'x7', level=0.95)
#          2.5 %      97.5 %
# x7  0.01185532   0.3760651
# (.012, .376)
# b. Find a 95% CI on the mean # of games won by a team when x2 = 2300, x7 = 56 and x8 = 2100
predict(lm_3.1, newdata = data.frame(x2 = 2300, x7 = 56, x8 = 2100), interval="confidence")
# fit: 7.216424
#       lwr        upr
#  6.436203   7.996645
# The 95% confidence interval of mean number of games won
# when x2 = 2300, x7 = 56 and x8 = 2100 is 6.4 - 7.99
##########################################
# 3.4
##########################################
##########################################
# use table b1 :: regress x7 and x9 against y
# a. test for significance of regression
lm_3.4 <- lm(y~x7 + x8, data=b1)
summary(lm_3.4)
#                Estimate   Std. Error     t value    Pr(>|t|)
# (Intercept)   17.944319     9.862484       1.819    0.08084 .
# x7             0.048371     0.119219       0.406    0.68839
# x8            -0.006537     0.001758      -3.719    0.00102 **
# Residual standard error: 2.432 on 25 degrees of freedom
# Multiple R-squared:  0.5477,	Adjusted R-squared:  0.5115
# F-statistic: 15.13 on 2 and 25 DF,  p-value: 4.935e-05
predict(lm_3.4, newdata = data.frame(x7 = 56, x8 = 2100), interval="confidence")
# 3.5
##########################################
# Use table B3
# a. Fit a reg model between mpg (y) and the displacement (x1) and # carburetor barrels (x6)
b3 = read.xls('/Users/bradyfowler/Documents/Fall Semester/Modeling_6021/HW3/data-table-B3.xls')
head(b3)
lm_3.5 <- lm(y~x1 + x6, data=b3)
summary(lm_3.5)
((955.72+18.59)/2)/9.08
qf(.95, df1=2, df2=22)
((955.72+18.59)/2)/9.08
b5 = read.xls('/Users/bradyfowler/Documents/Fall Semester/Modeling_6021/HW3/data-table-B5.xls')
head(b5)
# a. Fit multi reg of CO2 (y) to solvent (x6) and hydrogen (x7)
lm_3.8 <- lm(y~x6 + x7, data=b5)
summary(lm_3.8)
anova(lm_3.8)
confint(lm_3.8, 'x6', level=0.95)
#          2.5 %       97.5 %
# x6  0.01285196   0.02419204
## (0.013, 0.024)
## B7
confint(lm_3.8, 'x7', level=0.95)
#         2.5 %     97.5 %
# x7  0.1782076   4.193298
## (0.178, 4.193)
# e. Refit the model using only x6. Test for significance.
# Calculate R2 and R2adj. Discuss findings. Are you satisfied with this model?
lm_3.8.e <- lm(y~x6, data=b5)
summary(lm_3.8.e)
#               Estimate  Std. Error  t value   Pr(>|t|)
# (Intercept)   6.144181   3.483064     1.764     0.0899 .
# x6            0.019395   0.002932     6.616   6.24e-07 ***
# Residual standard error: 10.7 on 25 degrees of freedom
# Multiple R-squared:  0.6365,	Adjusted R-squared:  0.6219
# F-statistic: 43.77 on 1 and 25 DF,  p-value: 6.238e-07
## The x6 variable has a t-statistic of 6.6  with p-value = 6.24e-07 which is significant at the .05 level
## R2 for the regression is 63.65% and R2adj is 62.19%
## All of these values are roughly the same as when we included x7 in the model as well.
## Part b had R2 = 69.96% and R2adj = 67.46% which means it explained slightly more of
## the variance in y when x7 was included (and since in part b the x7 coefficient was significant
## at the .05 level it might be worth keeping in the regression).
## MSE of x6 only:
sum(lm_3.8.e$residuals^2)/anova(lm_3.8.e)[2,1]
## 114.447
## MSE of model with x6 and x7 only:
sum(lm_3.8$residuals^2)/anova(lm_3.8)[3,1]
## 98.49313
??## As expected, the MSE of the model with more variables is lower.
summary(lm_3.8)
##########################################
# 3.8
##########################################
# Use b5
b5 = read.xls('/Users/bradyfowler/Documents/Fall Semester/Modeling_6021/HW3/data-table-B5.xls')
head(b5)
# a. Fit multi reg of CO2 (y) to solvent (x6) and hydrogen (x7)
lm_3.8 <- lm(y~x6 + x7, data=b5)
summary(lm_3.8)
#             Estimate  Std. Error   t value    Pr(>|t|)
# (Intercept) 2.526460   3.610055     0.700       0.4908
# x6          0.018522   0.002747     6.742       5.66e-07 ***
# x7          2.185753   0.972696     2.247       0.0341 *
# Residual standard error: 9.924 on 24 degrees of freedom
# Multiple R-squared:  0.6996,	Adjusted R-squared:  0.6746
# F-statistic: 27.95 on 2 and 24 DF,  p-value: 5.391e-07
## y = 2.526460 + 0.018522 x6 + 2.185753 x7
# b. Test for significance of regression
# Calculate R2 and R2adj
anova(lm_3.8)
#           Df    Sum Sq   Mean Sq    F value       Pr(>F)
# x6         1    5008.9    5008.9    50.8557   2.267e-07 ***
# x7         1     497.3     497.3     5.0495      0.0341 *
# Residuals 24    2363.8      98.5
## From the stat summary: F value is 27.95 with p = .0000 (significant at the .05 level) so the regression is significant
## R2 = 69.96% and R2adj = 67.46% which indicates ~70% of the variance is explained by this model.
# c. Use t test to determine contribution of x6 and x7 to the model
## From the model summary:
# Coefficient   t value    Pr(>|t|)
#       x6        6.742    5.66e-07 ***
#       x7        2.247    0.0341 *
# Both variables are significant in the model at the .05 level.
# d. Construct 95% CI on B6 and B7
## B6
confint(lm_3.8, 'x6', level=0.95)
#          2.5 %       97.5 %
# x6  0.01285196   0.02419204
## (0.013, 0.024)
## B7
confint(lm_3.8, 'x7', level=0.95)
#         2.5 %     97.5 %
# x7  0.1782076   4.193298
## (0.178, 4.193)
# e. Refit the model using only x6. Test for significance.
# Calculate R2 and R2adj. Discuss findings. Are you satisfied with this model?
lm_3.8.e <- lm(y~x6, data=b5)
summary(lm_3.8.e)
#               Estimate  Std. Error  t value   Pr(>|t|)
# (Intercept)   6.144181   3.483064     1.764     0.0899 .
# x6            0.019395   0.002932     6.616   6.24e-07 ***
# Residual standard error: 10.7 on 25 degrees of freedom
# Multiple R-squared:  0.6365,	Adjusted R-squared:  0.6219
# F-statistic: 43.77 on 1 and 25 DF,  p-value: 6.238e-07
## y = 6.144181 + 0.019395x6
## From the stat summary: F value is 43.77 with p = 6.238e-07 (significant at the .05 level) so the regression is significant
## The x6 variable has a t-statistic of 6.6  with p-value = 6.24e-07 which is significant at the .05 level
## R2 for the regression is 63.65% and R2adj is 62.19%
## All of these values are roughly the same as when we included x7 in the model as well.
## Part b had R2 = 69.96% and R2adj = 67.46% which means it explained slightly more of
## the variance in y when x7 was included (and since in part b the x7 coefficient was significant
## at the .05 level it might be worth keeping in the regression).
## We know that if adding a variable does not significantly change the coefficient of other variables
## then the new variable (x7) is either uncorrelated with the existing betas or the response.
# f. Construct a 95% CI on B6 using the model fit in E. Compare this length
# to the lengths from part d. Does this tell you anything about the contribution of x7
confint(lm_3.8.e, 'x6', level=0.95)
summary(lm_3.8)
anova(lm_3.8)
anova(lm_3.8.e)
b7 = read.xls('/Users/bradyfowler/Documents/Fall Semester/Modeling_6021/HW3/data-table-B7.xls')
head(b7)
# a. Fit multi reg relating yield (y) to Co2 pressure (x1), Co2 temperature (x2),
# peanut moisture (x3), Co2 low rate (x4), peanut particle size (x5)
lm_3.11 <- lm(y ~ x1+x2+x3+x4+x5, data=b7)
summary(lm_3.11)
options(scipen = 999)
summary(lm_3.11)
options(scipen = 0)
anova(lm_3.11)
b16 = read.xls('/Users/bradyfowler/Documents/Fall Semester/Modeling_6021/HW3/data-table-B16.xls')
head(b16)
# a. Fit multi reg relating yield (y) to Co2 pressure (x1), Co2 temperature (x2),
# peanut moisture (x3), Co2 low rate (x4), peanut particle size (x5)
lm_3.16.LifeExp       <- lm(LifeExp       ~ People.per.TV + People.per.Dr, data=b16)
lm_3.16.LifeExpMale   <- lm(LifeExpMale   ~ People.per.TV + People.per.Dr, data=b16)
lm_3.16.LifeExpFemale <- lm(LifeExpFemale ~ People.per.TV + People.per.Dr, data=b16)
## Note: didnt paste full summary because it was unnecessary. Extracted coefficients from print out:
summary(lm_3.16.LifeExp)
head(b16)
mean(74,67)
sum(74+67)/2
setwd("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/output/")
load("uberx.Rda")
####################################################################################
## begin matching
####################################################################################
setwd("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/output/")
load("uberx.Rda")
## pull in latitude and longitude locations for each coordinate location
lat.long <- read.csv("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/Data/awsLocations.csv")
#plot to see distribution
ggplot(lat.long, aes(x=longitude, y=latitude)) +
geom_point() +
coord_equal()
library(dplyr); library(ggplot2); library(lubridate)
library(dplyr); library(ggplot2); library(lubridate); library(geosphere); library(plyr)
setwd("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/output/")
load("uberx.Rda")
## pull in latitude and longitude locations for each coordinate location
lat.long <- read.csv("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/Data/awsLocations.csv")
#plot to see distribution
ggplot(lat.long, aes(x=longitude, y=latitude)) +
geom_point() + coord_equal()
## join lat/long into uberx
## remove unnecessary columns and cast the timestamps
uber.x2  <- left_join(uber.x, lat.long, by=c("start_location_id"="locations")) %>%
select(-end_location_id, -expected_wait_time, -product_type)
uber.x2$timestamp <- ymd_hms(uber.x2$timestamp, tz = "UTC")
head(uber.x2)
## import crime data and fix column names
crime <- read.csv("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/Data/Crime_Incidents__2016.csv")
colnames(crime)<-tolower(c("long", "lat", colnames(crime)[3:length(crime)]))
nrow(crime)
# 22,223
# fix dates:
crime$reportdatetime    <- ymd_hms(crime$reportdatetime, tz = "UTC")
crime$lastmodifieddate  <- ymd_hms(crime$lastmodifieddate, tz = "UTC")
crime$start_date        <- ymd_hms(crime$start_date, tz = "UTC")
crime$end_date          <- ymd_hms(crime$end_date, tz = "UTC")
## look at offense scatter across DC
ggplot(crime, aes(x=long, y=lat)) +
geom_point() + coord_equal()
## look at offense type distribution
crime %>%
group_by(offense)  %>%
summarise(count=n()) %>%
mutate(freq = count / sum(count)) %>%
arrange(desc(count))
#                     offense count         freq
#                 THEFT/OTHER  8830 0.3973360932
#                THEFT F/AUTO  6615 0.2976645817
#                     ROBBERY  1966 0.0884669037
#         MOTOR VEHICLE THEFT  1614 0.0726274580
#  ASSAULT W/DANGEROUS WEAPON  1571 0.0706925258
#                    BURGLARY  1356 0.0610178644
#                   SEX ABUSE   181 0.0081447149
#                    HOMICIDE    86 0.0038698646
#                       ARSON     4 0.0001799937
# Probably only want to concentrate on non property crimes
## create function to find the nearest crime (if any) to a surge price time
## also make sure its pretty soon after
# extract items of interest from crime table
crime_list<- crime[,c("long", "lat", "reportdatetime", "objectid")]
# standard city block is ~250 meters. we want to look within 4 blocks
nearest_soonest <- function(row_num){
# details for current uber call
deets <- uber.x2[row_num, c("timestamp", "latitude", "longitude")]
# limit to crimes within an hour
potential_crimes <- crime_list[abs(difftime(deets[,1], crime_list$reportdatetime,units="hours")) < 1,]
# limit to crimes within 1000 meters
potential_crimes <- potential_crimes %>%
mutate(dist = by(potential_crimes, 1:nrow(potential_crimes), function(row) { distm(c(deets[,2], deets[,3]), c(row$lat, row$long), fun = distHaversine)  })) %>%
filter(dist<1000)
# return matched crimes
return(as.vector(potential_crimes$objectid))
}
# couldnt get the other sapply or mutate approach to work
# so here ill apply it in a loop
for (i in 1:nrow(uber.x2)) {
uber.x2$newvar[i] <- nearest_soonest(i)
}
for (i in 1:nrow(uber.x2)) {
uber.x2$[dist, i] <- nearest_soonest(i)
}
for (i in 1:nrow(uber.x2)) {
uber.x2[dist, i] <- nearest_soonest(i)
}
for (i in 1:nrow(uber.x2)) {
uber.x2[i, dist] <- nearest_soonest(i)
}
nearest_soonest <- function(row_num){
# details for current uber call
deets <- uber.x2[row_num, c("timestamp", "latitude", "longitude")]
# limit to crimes within an hour
potential_crimes <- crime_list[abs(difftime(deets[,1], crime_list$reportdatetime,units="hours")) < 1,]
# limit to crimes within 1000 meters
potential_crimes <- potential_crimes %>%
mutate(dist = by(potential_crimes, 1:nrow(potential_crimes), function(row) { distm(c(deets[,2], deets[,3]), c(row$lat, row$long), fun = distHaversine)  })) %>%
filter(dist<1000)
# return matched crimes
return(as.list(potential_crimes$objectid))
}
for (i in 1:nrow(uber.x2)) {
uber.x2[i, dist] <- nearest_soonest(i)
}
# standard city block is ~250 meters. we want to look within 4 blocks
nearest_soonest <- function(row_num){
# details for current uber call
deets <- uber.x2[row_num, c("timestamp", "latitude", "longitude")]
# limit to crimes within an hour
potential_crimes <- crime_list[abs(difftime(deets[,1], crime_list$reportdatetime,units="hours")) < 1,]
# limit to crimes within 1000 meters
potential_crimes <- potential_crimes %>%
mutate(dist = by(potential_crimes, 1:nrow(potential_crimes), function(row) { distm(c(deets[,2], deets[,3]), c(row$lat, row$long), fun = distHaversine)  })) %>%
filter(dist<1000)
# return matched crimes
return(list(potential_crimes$objectid))
}
# couldnt get the other sapply or mutate approach to work
# so here ill apply it in a loop
for (i in 1:nrow(uber.x2)) {
uber.x2[i, dist] <- nearest_soonest(i)
}
nearest_soonest(3)
# so here ill apply it in a loop
for (i in 1:nrow(uber.x2)) {
uber.x2$dist[i] <- nearest_soonest(i)
}
head(uber.x2)
head(uber.x2)
View(uber.x2)
xx<-head(uber.x2)
xx
floor(minute(xx$timestamp)/30)*30
minute(xx$timestamp)
minute(xx$timestamp)<- floor(minute(xx$timestamp)/30)*30
xx
second(xx$timestamp)<-0
xx
minute(uber.x2$timestamp.half)<- floor(minute(xx$timestamp)/30)*30
minute(uber.x2$timestamp.half)<- floor(minute(uber.x2$timestamp)/30)*30
minute(uber.x2$timestamp)
floor(minute(uber.x2$timestamp)/30)*30
uber.x2$timestamp.half <- uber.x2$timestamp
minute(uber.x2$timestamp.half)<- floor(minute(uber.x2$timestamp)/30)*30
second(uber.x2$timestamp.half)<-0
rm(uber.x)
head(uber.x2)
uber.x2.roll <- uber.x2 %>%
groupby(timestamp.half, start_location_id, latitude, longitude) %>%
mutate(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
uber.x2.roll <- uber.x2 %>%
group_by(timestamp.half, start_location_id, latitude, longitude) %>%
mutate(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
uber.x2.roll <- uber.x2 %>%
select(-timestamp, -dist) %>%
group_by(timestamp.half, start_location_id, latitude, longitude) %>%
mutate(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
head(uber.x2.roll)
uber.x2.roll <- uber.x2 %>%
select(-timestamp, -dist) %>%
group_by(timestamp.half, start_location_id, latitude, longitude) %>%
summarise(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
head(uber.x2.roll)
uber.x2.roll <- uber.x2 %>%
group_by(timestamp.half, start_location_id, latitude, longitude) %>%
summarise(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
head(uber.x2)
uber.x2 %>%
group_by(timestamp.half, start_location_id, latitude, longitude)
uber.x2.roll <- uber.x2 %>%
group_by(timestamp.half, start_location_id, latitude, longitude) %>%
summarise(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
uber.x2.roll <- uber.x2 %>%
group_by(timestamp.half, start_location_id, latitude, longitude) %>%
mutate(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
uber.x2.roll <- uber.x2 %>%
group_by(timestamp.half, start_location_id, latitude, longitude) %>%
select(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
head(uber.x2)
uber.x2.roll <- uber.x2 %>%
dplyr::group_by(timestamp.half, start_location_id, latitude, longitude) %>%
dplyr::summarise(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
head(uber.x2)
uber.x2.roll <- uber.x2 %>%
dplyr::select(-timestamp, -dist) %>%
dplyr::group_by(timestamp.half, start_location_id, latitude, longitude) %>%
dplyr::summarise(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
head(uber.x2)
uber.x2.roll <- uber.x2 %>%
dplyr::group_by(timestamp.half, start_location_id, latitude, longitude) %>%
dplyr::summarise(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE)) %>%
dplyr::select(-timestamp, -dist)
uber.x2.roll <- uber.x2 %>%
dplyr::group_by(timestamp.half, start_location_id, latitude, longitude) %>%
dplyr::summarise(avg.surge = mean(surge_multiplier, na.rm=TRUE),
avg.low   = mean(low_estimate, na.rm=TRUE),
avg.high  = mean(high_estimate, na.rm=TRUE))
head(uber.x2)
head(uber.x2.roll)
tail(uber.x2.roll)
for (i in 1:nrow(uber.x2.roll)) {
uber.x2.roll$dist[i] <- nearest_soonest(i)
}
nearest_soonest <- Vectorize(function(row_num){
# details for current uber call
deets <- uber.x2[row_num, c("timestamp", "latitude", "longitude")]
# limit to crimes within an hour
potential_crimes <- crime_list[abs(difftime(deets[,1], crime_list$reportdatetime,units="hours")) < 1,]
# limit to crimes within 1000 meters
potential_crimes <- potential_crimes %>%
mutate(dist = by(potential_crimes, 1:nrow(potential_crimes), function(row) { distm(c(deets[,2], deets[,3]), c(row$lat, row$long), fun = distHaversine)  })) %>%
filter(dist<1000)
# return matched crimes
return(list(potential_crimes$objectid))
})
