summarise(n()>1)
uber.crime %>% filter(timestamp =="2016-02-08 18:00:00")
uber.crime %>% filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300")
uber.crime %>% filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300") %>% head()
uber.crime %>% filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300") %>% all()
uber.crime %>% filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300")
uber.crime %>% filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300") %>% head(5)
uber.crime %>% filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300") ->x
x
x  %>% glimpse()
uber.pooled.census %>% filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300")
uber.pooled %>% filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300")
uber.pooled.census %>% filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300")
uber.pooled.census %>%
filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300") %>%
glimpse()
add.census.tract
uber.locations %>% add.census.tract
uber.crime %>% glimpse()
suppressWarnings(suppressMessages(library(dplyr)))
# # install.packages("devtools")
# devtools::install_github("hadley/multidplyr")
library(multidplyr)
suppressMessages(library(lubridate))
library(readr)
paste("Starting Processing ...", Sys.time())
## read uber data in. downloaded from:
## https://drive.google.com/drive/u/0/folders/0B-mutxqHY34rblhORk9raWxQQjQ
uber <- read_csv("../Data/TractsSurgeDC2_Feb4_Mar2.csv")
## data doc:
# "timestamp" : string, Date and Time (EST) when API was pinged
uber <- read_csv("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/uberdata/TractsSurgeDC2_Feb4_Mar2.csv")
paste("Loaded CSV ...", Sys.time())
# pool together all types of ubers in case we decide to look at it this way
uber %>%
saveRDS(file="../output/uber.rds") # RDS serializes the data and prevents namespace pollution on readRDS
paste("Done", Sys.time())
census.tracts <- readShapePoly('../Data/Census/Census_Tracts__2010.shp')
census.data <- read.csv('../Data/Census/Census_Tracts__2010.csv')
# uber <- readRDS("../Output/uber.rds")
uber.locations <- read_csv("../Data/awsLocations.csv")
crime.data <- read_csv("../Data/Crime_Incidents__2016.csv") %>%
filter(
start_date > min(uber.pooled$timestamp),
start_date < max(uber.pooled$timestamp),
!is.na(end_date)
) %>%
select(longitude, latitude, offense, ward, district, census_tract, start_date, end_date)
################################################################################
#
# Census Location Tools
#
################################################################################
tract_geom <- fortify(census.tracts, region = "GEOID")
tract_poly <- merge(tract_geom, census.data, by.x = "id", by.y = "GEOID")
assign.census.tract.id <- function(crime.data.current,ids) {
if (length(ids) < 1) return(crime.data.current)
id.current <- ids %>% first
id.tail <- ids %>% tail(length(ids)-1)
pnt.in.poly(
crime.data.current %>% select(longitude, latitude),
tract_geom %>% filter(id==id.current) %>% select(long,lat)
) %>% select(pip) %>% unlist -> pip
crime.data.current %>%
mutate(census.tract=ifelse(pip==1,id.current,census.tract)) %>%
assign.census.tract.id(id.tail)
}
add.census.tract <- function(long.lat.data) {
tract_geom %>%
get('id',.) %>%
unique -> tract_geom.ids
long.lat.data %>%
mutate(census.tract=NA) %>%
assign.census.tract.id(tract_geom.ids)
}
# super fast 'cheating'
crime.data.add.census.tract <- function(crime.data) {
crime.data %>%
mutate(census.tracts=paste('11001',census_tract,sep='')) %>%
select(-census_tract)
}
################################################################################
#
# Combine and Group By Census Tracts
#
################################################################################
left_join(uber.pooled,
uber.locations %>% add.census.tract,
by=c("start_location_id"="locations")) %>%
select(-start_location_id) -> uber.pooled.census
left_join(uber,
uber.locations %>% add.census.tract,
by=c("start_location_id"="locations")) %>%
select(-start_location_id) -> uber.census
uber.census %>% glimpse()
uber.census %>%
mutate(timestamp=update(timestamp,minutes=0,seconds=0)) %>%
partition(timestamp, census.tract) %>%
summarise(avg_surge_multiplier=mean(as.numeric(surge_multiplier), na.rm=TRUE),
avg_expected_wait_time=mean(as.numeric(expected_wait_time), na.rm=TRUE),
avg_low_estimate=mean(as.numeric(low_estimate), na.rm=TRUE),
avg_high_estimate=mean(as.numeric(high_estimate ), na.rm=TRUE)) %>%
collect() %>%
saveRDS(file="../output/uber.pooled.census.rds")
suppressMessages(library(dplyr))
suppressMessages(library(lubridate))
suppressMessages(library(deldir))
suppressMessages(library(ks))
suppressMessages(library(purrr))
library(multidplyr)
library(mapproj)
library(ggplot2)
library(readr)
library(SDMTools)
library(maptools)
install.package(mapproj)
install.packages("mapproj")
install.packages("multidplyr")
install.packages("installr") # install
setInternet2(TRUE) # only for R versions older than 3.3.0
installr::updateR() # updating R.
install.packages("installr") # install
install.packages("multidplyr")
library(multidplyr)
install.packages("devtools")
devtools::install_github("hadley/multidplyr")
library(multidplyr)
# Collect and combine the datasets
suppressMessages(library(dplyr))
suppressMessages(library(lubridate))
suppressMessages(library(deldir))
suppressMessages(library(ks))
suppressMessages(library(purrr))
# # install.packages("devtools")
# devtools::install_github("hadley/multidplyr")
library(multidplyr)
library(mapproj)
library(ggplot2)
library(readr)
library(SDMTools)
library(maptools)
suppressMessages(library(dplyr))
suppressMessages(library(lubridate))
suppressMessages(library(deldir))
suppressMessages(library(ks))
suppressMessages(library(purrr))
suppressMessages(library(mapproj))
# # install.packages("devtools")
# devtools::install_github("hadley/multidplyr")
library(multidplyr)
library(mapproj)
library(ggplot2)
library(readr)
library(SDMTools)
library(maptools)
################################################################################
census.tracts <- readShapePoly('../Data/Census/Census_Tracts__2010.shp')
census.data <- read.csv('../Data/Census/Census_Tracts__2010.csv')
# uber <- readRDS("../Output/uber.rds")
uber.locations <- read_csv("../Data/awsLocations.csv")
crime.data <- read_csv("../Data/Crime_Incidents__2016.csv") %>%
filter(
start_date > min(uber.pooled$timestamp),
start_date < max(uber.pooled$timestamp),
!is.na(end_date)
) %>%
select(longitude, latitude, offense, ward, district, census_tract, start_date, end_date)
################################################################################
#
# Census Location Tools
#
################################################################################
tract_geom <- fortify(census.tracts, region = "GEOID")
tract_poly <- merge(tract_geom, census.data, by.x = "id", by.y = "GEOID")
assign.census.tract.id <- function(crime.data.current,ids) {
if (length(ids) < 1) return(crime.data.current)
id.current <- ids %>% first
id.tail <- ids %>% tail(length(ids)-1)
pnt.in.poly(
crime.data.current %>% select(longitude, latitude),
tract_geom %>% filter(id==id.current) %>% select(long,lat)
) %>% select(pip) %>% unlist -> pip
crime.data.current %>%
mutate(census.tract=ifelse(pip==1,id.current,census.tract)) %>%
assign.census.tract.id(id.tail)
}
add.census.tract <- function(long.lat.data) {
tract_geom %>%
get('id',.) %>%
unique -> tract_geom.ids
long.lat.data %>%
mutate(census.tract=NA) %>%
assign.census.tract.id(tract_geom.ids)
}
# super fast 'cheating'
crime.data.add.census.tract <- function(crime.data) {
crime.data %>%
mutate(census.tracts=paste('11001',census_tract,sep='')) %>%
select(-census_tract)
}
################################################################################
#
# Combine and Group By Census Tracts
#
################################################################################
## Read in full Uber file
uber <- read_csv("../Data/TractsSurgeDC2_Feb4_Mar2.csv")
left_join(uber,
uber.locations %>% add.census.tract,
uber <- read_csv("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/uberdata/TractsSurgeDC2_Feb4_Mar2.csv")
>
x
)
uber <- read_csv("/Users/bradyfowler/Documents/Fall Semester/Mining_6018/case1-crime/uberdata/TractsSurgeDC2_Feb4_Mar2.csv")
uber.census %>%
mutate(timestamp=update(timestamp,minutes=0,seconds=0)) %>%
partition(timestamp, census.tract) %>%
summarise(avg_surge_multiplier=mean(as.numeric(surge_multiplier), na.rm=TRUE),
avg_expected_wait_time=mean(as.numeric(expected_wait_time), na.rm=TRUE),
avg_low_estimate=mean(as.numeric(low_estimate), na.rm=TRUE),
avg_high_estimate=mean(as.numeric(high_estimate ), na.rm=TRUE)) %>%
collect() -> uber.pooled.census
uber.pooled.census %>%
filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300") %>%
glimpse()
uber.census %>%
mutate(timestamp=update(timestamp,minutes=0,seconds=0)) %>%
partition(timestamp, census.tract) %>%
summarise(avg_surge_multiplier=mean(as.numeric(surge_multiplier), na.rm=TRUE),
avg_expected_wait_time=mean(as.numeric(expected_wait_time), na.rm=TRUE),
avg_low_estimate=mean(as.numeric(low_estimate), na.rm=TRUE),
avg_high_estimate=mean(as.numeric(high_estimate ), na.rm=TRUE)) %>%
collect() -> uber.pooled.census
uber.pooled.census %>% saveRDS(file="../output/uber.pooled.census.rds")
## something is wrong with this combination - it has multiple rows per timestamp/census tract
uber.pooled.census %>%
filter(timestamp =="2016-02-08 18:00:00", census.tract=="11001000300") %>%
glimpse()
crime.data %>% add.census.tract -> crime.data.census
crime.data.census %>% saveRDS(file="../output/crime.data.census.rds")
uber.pooled.census <- readRDS("../Output/uber.pooled.census.rds")
crime.data.census <- readRDS("../Output/crime.data.census.rds") %>%
mutate(timestamp=update(start_date,minutes=0,seconds=0))
left_join(uber.pooled.census,
crime.data.census %>%
group_by(timestamp, census.tract) %>%
summarise(cnt.crimes=n()),
by=c("timestamp", "census.tract")) -> uber.crime
rm(uber)
uber.pooled.census <- readRDS("../Output/uber.pooled.census.rds")
crime.data.census <- readRDS("../Output/crime.data.census.rds") %>%
mutate(timestamp=update(start_date,minutes=0,seconds=0))
# uber.pooled.census$tmp <- paste(month(uber.pooled.census$timestamp),'-',day(uber.pooled.census$timestamp),'-',hour(uber.pooled.census$timestamp))
# crime.data.census$tmp <- paste(month(crime.data.census$timestamp),'-',day(crime.data.census$timestamp),'-',hour(crime.data.census$timestamp))
# join crimes into uber data on the hourly level
left_join(uber.pooled.census,
crime.data.census %>%
group_by(timestamp, census.tract) %>%
summarise(cnt.crimes=n()),
by=c("timestamp", "census.tract")) -> uber.crime
unique(uber.crime$cnt.crimes, na.rm=TRUE) == nrow(crime.data.census)
sum(uber.crime$cnt.crimes, na.rm=TRUE) == nrow(crime.data.census)
sum(uber.crime$cnt.crimes, na.rm=TRUE)
nrow(crime.data.census)
uber.crime %>%
group_by(timestamp, census.tract) %>%
summarise(cnt=n()) %>%  filter(cnt>1)
uber.crime %>%
group_by(cnt.crimes) %>%
summarise(cnt=n())
sum(uber.crime$cnt.crimes, na.rm=TRUE)
crime.data.census %>%
group_by(timestamp, census.tract) %>%
summarise(cnt.crimes=n()) %>%
group_by(cnt.crimes) %>%
summarise(cnt=n())
## this will make sure crimes are only counted once
sum(uber.crime$cnt.crimes, na.rm=TRUE)
nrow(crime.data.census)
# not exactly joined 1:1 for some reason?
uber.crime %>% glimpse()
lm <- lm(cnt.crimes ~ avg_surge_multiplier, data=uber.crime)
summary(lm)
lm <- lm(cnt.crimes ~ avg_surge_multiplier, factor(census.tract), data=uber.crime)
summary(lm)
lm <- lm(cnt.crimes ~ avg_surge_multiplier, factor(census.tract), data=uber.crime)
summary(lm)
factor(census.tract)
uber.crime %>% glimpse()
lm <- lm(cnt.crimes ~ avg_surge_multiplier, avg_expected_wait_time, data=uber.crime)
summary(lm)
lm <- lm(cnt.crimes ~ avg_surge_multiplier+avg_expected_wait_time, data=uber.crime)
summary(lm)
lm <- lm(cnt.crimes ~ avg_surge_multiplier+avg_expected_wait_time+factor(census.tract), data=uber.crime)
summary(lm)
lm <- lm(cnt.crimes ~
avg_surge_multiplier+
avg_expected_wait_time+
factor(hour(timestamp)), data=uber.crime)
summary(lm)
lm <- lm(cnt.crimes ~
avg_surge_multiplier+
avg_expected_wait_time+
factor(census.tract), data=uber.crime)
summary(lm)
library(DAAG)
lm <- lm(cnt.crimes ~
avg_surge_multiplier+
avg_expected_wait_time+
factor(census.tract), data=uber.crime)
cv.lm(df=uber.crime, lm, m=3)
cv.lm(df=uber.crime, cnt.crimes ~
avg_surge_multiplier+
avg_expected_wait_time+
factor(census.tract), m=3)
linear.reg <- lm(uber.crime$cnt.crimes ~
uber.crime$avg_surge_multiplier+
uber.crime$avg_expected_wait_time+
factor(uber.crime$census.tract),)
linear.reg <- lm(uber.crime$cnt.crimes ~
uber.crime$avg_surge_multiplier+
uber.crime$avg_expected_wait_time+
factor(uber.crime$census.tract))
summary(linear.reg)
cv.lm(df=uber.crime, linear.reg, m=3)
uber.crime-> sam
cv.lm(df=sam, linear.reg, m=3)
linear.reg <- lm(cnt.crimes ~
avg_surge_multiplier+
avg_expected_wait_time+
factor(census.tract), data=uber.crime)
summary(linear.reg)
log.reg <- lm(num.crimes>0 ~
avg_surge_multiplier+
avg_expected_wait_time+
factor(census.tract), data=uber.crime)
log.reg <- lm(cnt.crimes>0 ~
avg_surge_multiplier+
avg_expected_wait_time+
factor(census.tract), data=uber.crime)
summary(log.reg)
log.reg <- lm(cnt.crimes>0 ~
avg_surge_multiplier+
avg_expected_wait_time, data=uber.crime)
summary(log.reg)
train.indices = sample(1:nrow(uber.crime), as.integer(nrow(uber.crime) * 0.75))
pairs(uber.crime[,c("cnt.crimes","avg_surge_multiplier", "avg_expected_wait_time")])
uber.crime$has.crime <- uber.crime$cnt.crimes>0
predictions = predict(lm, newdata = uber.crime[-train.indices, ], type="response")
predictions = predict(linear.reg, newdata = uber.crime[-train.indices, ], type="response")
log.reg <- glm(cnt.crimes>0 ~
avg_surge_multiplier+
avg_expected_wait_time, data=uber.crime, family="binomial")
log.reg <- glm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time, data=uber.crime, family="binomial")
log.reg <- glm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time, data=uber.crime, family=binomial)
log.reg <- glm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time, data=uber.crime, family="binomial")
head(has.crime)
uber.crime$has.crime <- ifelse(uber.crime$cnt.crimes>0,1,0)
train.indices = sample(1:nrow(uber.crime), as.integer(nrow(uber.crime) * 0.75))
#pairs(uber.crime[,c("cnt.crimes","avg_surge_multiplier", "avg_expected_wait_time")])
log.reg <- glm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time, data=uber.crime, family="binomial")
log.reg <- lm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time, data=uber.crime, family="binomial")
summary(log.reg)
log.reg <- lm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time,
factor(census.tract), data=uber.crime, family="binomial")
summary(log.reg)
log.reg <- lm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time,
factor(census.tract), data=uber.crime, family="binomial")
log.reg <- lm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time,
factor(census.tract), data=uber.crime)
summary(log.reg)
uber.crime %>% glimpse()
uber.crime[is.na(has.crime)] <- 0
uber.crime[is.na(uber.crime$cnt.crimes)] <- 0
uber.crime[is.na(cnt.crimes)] <- 0
uber.crime[is.na(cnt.crimes)]
# join crimes into uber data on the hourly level
left_join(uber.pooled.census,
crime.data.census %>%
group_by(timestamp, census.tract) %>%
summarise(cnt.crimes=n()),
by=c("timestamp", "census.tract")) -> uber.crime
uber.crime[is.na(cnt.crimes)] <- 0
uber.crime$cnt.crimes[is.na(cnt.crimes$cnt.crimes)] <- 0
replace(uber.crime$cnt.crimes, is.na(uber.crime$cnt.crimes), "000/000")
replace(uber.crime$cnt.crimes, is.na(uber.crime$cnt.crimes), 0)
summary(replace(uber.crime$cnt.crimes, is.na(uber.crime$cnt.crimes), 0))
uber.crime$cnt.crimes <- replace(uber.crime$cnt.crimes, is.na(uber.crime$cnt.crimes), 0)
uber.crime$has.crime <- ifelse(uber.crime$cnt.crimes>0,1,0)
train.indices = sample(1:nrow(uber.crime), as.integer(nrow(uber.crime) * 0.75))
uber.crime %>% glimpse()
log.reg <- lm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time,
factor(census.tract), data=uber.crime), family="binomial")
log.reg <- lm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time,
factor(census.tract), data=uber.crime, family="binomial")
summary(log.reg)
log.reg <- glm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time,
factor(census.tract), data=uber.crime, family="binomial")
summary(log.reg)
log.reg <- glm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time+
factor(census.tract), data=uber.crime, family="binomial")
summary(log.reg)
pairs(uber.crime[,c("has.crime","avg_surge_multiplier", "avg_expected_wait_time")])
log.reg <- glm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time+
factor(census.tract), data=uber.crime[-train.indices, ], family="binomial")
log.reg <- glm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time+
factor(census.tract), data=uber.crime[train.indices, ], family="binomial")
summary(log.reg)
predictions = predict(linear.reg, newdata = uber.crime[-train.indices, ], type="response")
predictions = predict(log.reg, newdata = uber.crime[-train.indices, ], type="response")
predictions %>% glimpse()
confusionMatrix(predictions, uber.crime[-train.indices,]$has.crime)
library(caret)
confusionMatrix(predictions, uber.crime[-train.indices,]$has.crime)
uber.crime[-train.indices,]$has.crime
nrow(uber.crime[-train.indices,]$has.crime)
length(uber.crime[-train.indices,]$has.crime)
length(predictions)
confusionMatrix(predictions, uber.crime[-train.indices,]$has.crime)
head(predictions)
predictions[1]
predictions[[1]]
pred      <- factor( ifelse(predictions > threshold, 1, 0) )
threshold <- 0.5
pred      <- factor( ifelse(predictions > threshold, 1, 0) )
confusionMatrix(pred, uber.crime[-train.indices,]$has.crime)
threshold <- 0.2
pred      <- factor( ifelse(predictions > threshold, 1, 0) )
confusionMatrix(pred, uber.crime[-train.indices,]$has.crime)
threshold <- 0.1
pred      <- factor( ifelse(predictions > threshold, 1, 0) )
confusionMatrix(pred, uber.crime[-train.indices,]$has.crime)
matrix<- confusionMatrix(pred, uber.crime[-train.indices,]$has.crime)
matrix$table
threshold <- 0.2
pred      <- factor( ifelse(predictions > threshold, 1, 0) )
matrix <- confusionMatrix(pred, uber.crime[-train.indices,]$has.crime)
matrix$table
threshold <- 0.1
pred      <- factor( ifelse(predictions > threshold, 1, 0) )
matrix <- confusionMatrix(pred, uber.crime[-train.indices,]$has.crime)
matrix$table
threshold <- 0.05
pred      <- factor( ifelse(predictions > threshold, 1, 0) )
matrix <- confusionMatrix(pred, uber.crime[-train.indices,]$has.crime)
matrix$table
threshold <- 0.09
pred      <- factor( ifelse(predictions > threshold, 1, 0) )
matrix <- confusionMatrix(pred, uber.crime[-train.indices,]$has.crime)
matrix$table
threshold <- 0.1
pred      <- factor( ifelse(predictions > threshold, 1, 0) )
matrix <- confusionMatrix(pred, uber.crime[-train.indices,]$has.crime)
matrix$table
train.indices = sample(1:nrow(uber.crime), as.integer(nrow(uber.crime) * 0.75))
pairs(uber.crime[,c("has.crime","avg_surge_multiplier", "avg_expected_wait_time")])
## run model
log.reg <- glm(has.crime ~
avg_surge_multiplier+
factor(census.tract), data=uber.crime[train.indices, ], family="binomial")
avg_expected_wait_time+
summary(log.reg)
avg_expected_wait_time+
summary(log.reg)
avg_surge_multiplier+
factor(census.tract), data=uber.crime[train.indices, ], family="binomial")
>
>
log.reg <- glm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time+
factor(census.tract), data=uber.crime[train.indices, ], family="binomial")
summary(log.reg)
# predict on held-out 25% and evaluate confusion matrix
library(caret)
predictions = predict(log.reg, newdata = uber.crime[-train.indices, ], type="response")
threshold <- 0.1
pred      <- factor( ifelse(predictions > threshold, 1, 0) )
matrix <- confusionMatrix(pred, uber.crime[-train.indices,]$has.crime)
matrix$table
log.reg <- glm(has.crime ~
avg_surge_multiplier+
avg_expected_wait_time, data=uber.crime[train.indices, ], family="binomial")
summary(log.reg)
